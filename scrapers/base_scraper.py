import json
from abc import ABC, abstractmethod
from crawl4ai import CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from urllib.parse import urljoin
import logging
from pathlib import Path

from openai.resources import AsyncEmbeddings

from DataAccessLayer import ProductDataAccessLayer


class BaseScraper(ABC):
    def __init__(self, config , crawler, md_generator,website):
        self.config = config
        self.base_url = config['base_url']
        self.schema = self._load_schemas(config['schema_path'])
        self.logger = logging.getLogger(__name__)
        self.crawler = crawler
        self.md_generator = md_generator
        self.website_products = []
        self.dal = ProductDataAccessLayer(website)

    def _save_json(self, data, filename):
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    # run scraper with the provided configuration
    async def run(self):
        self.logger.info(f"Running scraper for {self.__class__.__name__}")
        try:
            brands = await self.get_brands()
            self._save_json(brands, f'brands_{self.__class__.__name__}.json')

            if not brands:
                self.logger.error("No brands found.")
                return

            visited_brands = set()
            for brand in brands[2:3]:# Remove [:] to scrape all brands
                self.logger.info(f"Scraping brand: {brand['BrandName']}")
                brand_url = urljoin(self.base_url, brand['BrandURL'])

                # Check if brand has already been visited
                if brand_url in visited_brands:
                    self.logger.info(f"Brand {brand['BrandName']} already visited. Skipping.")
                    continue
                visited_brands.add(brand_url)

                # Get product URLs from brand page
                product_urls = await self.get_product_urls_from_brand(brand_url)
                if not product_urls:
                    self.logger.exception(f"No products found for brand: {brand['BrandName']}")
                    continue
                self._save_json(product_urls, f'products_pages_{self.__class__.__name__}.json')

                for product_url in product_urls:
                    # Extract product details
                    product_url['ProductURL'] = urljoin(self.base_url, product_url['ProductURL'])
                    await self.parse_product(product_url['ProductURL'], brand['BrandName'])
            self._save_json(self.website_products, f'products_pages_{self.__class__.__name__}.json')

        except Exception as e:
            self.logger.exception(f"Error occurred: {e}")

        finally:
            self.logger.info(f"Saving Products Generated by {self.__class__.__name__}")
            return self.website_products

    def _save_product(self, product):
        """Save product to the database or file"""
        # Implement saving logic here
        self.website_products.extend(product)


    def _load_schemas(self, schema_path):
        """Load all schema from their respective files"""
        try:
            with open(Path.joinpath(Path.cwd() , schema_path)) as f:
                schema = json.load(f)
        except FileNotFoundError:
            raise Exception(f"Schema file not found: {Path.joinpath(Path.cwd() , schema_path)}")
        except json.JSONDecodeError:
            raise Exception(f"Invalid JSON in schema: {schema_path}")
        return schema

    async def get_brands(self):
        schema = self.schema['brands_urls_schema']
        url = urljoin(self.base_url, self.config['brands_url'])

        result = await self.crawler.arun(
            url=url,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema=schema , verbose=True)
            )
        )
        if not result:
            logging.error(f"Failed to extract brands from {url}")
            return []

        # Extract brand data
        extracted_brands = json.loads(result.extracted_content) if result.extracted_content else []

        self.logger.info(f"Successfully extracted {len(extracted_brands)} brands from {url}")
        return extracted_brands

    def clean_product_data(self, product):
        """Clean product data"""
        # Implement cleaning logic here
        pass

    @abstractmethod
    async def get_product_urls_from_brand(self, brand_url: str):
        """Get all product URLs from a brand page"""
        self.logger.info(f"Scraping products using: {self.__class__.__name__} scraper.")


    @abstractmethod
    async def parse_product(self, product_url: str, brand: str ):
        self.logger.info(f"Scraping product URL: {product_url} for brand: {brand}")

class InfiniteScrollScraper(BaseScraper):
    async def get_product_urls_from_brand(self, brand_url: str):
        """Get all product URLs from a brand page using infinite scroll"""
        # The crawler will handle the infinite scroll and return all product URLs
        await super().get_product_urls_from_brand(brand_url)
        return await self.get_products_pages_urls(brand_url)

    async def get_products_pages_urls(self, url: str):
        """Get all product URLs from a brand page using infinite scroll"""
        result = await self.crawler.arun(
            url=url,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema=self.schema['products_urls_schema']),
                scan_full_page=True,  # Enables scrolling
                scroll_delay=0.2  # Waits 200ms between scrolls
            )
        )
        return json.loads(result.extracted_content) if result.extracted_content else []



class NextPageButtonScraper(BaseScraper):
    async def get_product_urls_from_brand(self, brand_url: str):
        """Get all product URLs from a brand page using next page button"""
        await super().get_product_urls_from_brand(brand_url)
        product_urls = []
        current_url = brand_url

        while current_url:
            # Get products
            products_result = await self.get_products_pages_urls(current_url)
            if products_result:
                product_urls.extend(products_result)
            self.logger.info(f"Found {len(products_result)} products in {current_url}")

            # Get next page
            current_url = await self._get_next_page_url(current_url)

        return product_urls

    async def get_products_pages_urls(self, url: str):

        result = await self.crawler.arun(
            url=url,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema=self.schema['products_urls_schema'])
            )
        )
        return json.loads(result.extracted_content) if result.extracted_content else []

    async def _get_next_page_url(self, current_url: str):

        result = await self.crawler.arun(
            url=current_url,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema=self.schema['pagination_schema'])
            )
        )
        data = json.loads(result.extracted_content) if result.extracted_content else []
        next_url = data[0].get('next_page_url') if data else None
        return urljoin(current_url, next_url) if next_url else None


class LoadMoreScraper(BaseScraper):
    async def get_product_urls_from_brand(self, brand_url: str):
        base_url = brand_url
        current_url = brand_url
        load_more_quey = self.config['load_more_query']
        load_more_offset = self.config['load_more_offset']
        cur_offset = self.config['load_more_offset']
        product_urls = []

        while await self._has_more_products(current_url):
            cur_page_product_urls = await self.get_products_pages_urls(current_url)
            product_urls.extend(cur_page_product_urls)
            self.logger.info(f"Found {len(cur_page_product_urls)}/{len(product_urls)} products in {current_url}")
            cur_offset += load_more_offset
            current_url = urljoin(base_url , f'{load_more_quey}{cur_offset}')
            self.logger.debug(f"Successful pagination to offset {cur_offset}: {current_url}")

        cur_page_product_urls = await self.get_products_pages_urls(current_url)
        product_urls.extend(cur_page_product_urls)
        self.logger.info(f"Found {len(product_urls)} products in {current_url}")
        return product_urls

    async def get_products_pages_urls(self, url: str):

        try:
            result = await self.crawler.arun(
                url=url,
                config=CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    extraction_strategy=JsonCssExtractionStrategy(schema=self.schema['products_urls_schema'])
                )
            )
        except Exception as e:
            self.logger.error(f"Error occurred while scraping products: {e}")
            raise Exception('Error occurred while scraping products in get_product_urls_from_brand function\
             in LoadMoreScraper class may be the wait condition is not met')
        data = json.loads(result.extracted_content) if result.extracted_content else []

        return data

    async def _has_more_products(self, url: str):
        # Implement check for load more button visibility
        self.logger.debug(f"Checking for more products at: {url}")
        has_more_button = await self.crawler.arun(
            url=url,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema=self.schema["pagination_schema"])
            )
        )

        more_button_result = json.loads(has_more_button.extracted_content) if has_more_button.extracted_content else None
        return more_button_result

    async def _load_more_products(self, url: str):
        # Implement load more button click simulation
        pass


